{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author = 'Timothy McDonough'\n",
    "### Email = 'timothylmcdonough@gmail.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Modeling the Data  \n",
    "### 4a. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries to perform preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a \"Data\" class to create functions needed to prepare the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, train_feature_df, train_target_df, test_df, cat_vars, num_vars, target_var, id_col):\n",
    "        '''create train and test dataframes'''\n",
    "        #create new copies instead of references\n",
    "        self.cat_vars = list(cat_vars)\n",
    "        self.num_vars = list(num_vars)\n",
    "        self.feature_vars = cat_vars + num_vars\n",
    "        self.target_col = target_var\n",
    "        self.id_col = id_col\n",
    "        self.label_encoders = {}\n",
    "        self.train_df = self._create_train_df(train_feature_df, train_target_df)\n",
    "        self.test_df = self._create_test_df(test_df)\n",
    "    \n",
    "    def label_encode_df(df, cols):\n",
    "        '''creates one label encoder for each column in the data object instance'''\n",
    "        for col in cols:\n",
    "            if col in label_encoders:\n",
    "                #if label encoder already exits for col, use it\n",
    "                _label_encode(df, col, label_encoders[col])\n",
    "            else:\n",
    "                _label_encode(df, col)\n",
    "    \n",
    "    def inverse_encode_df(self, df, cols):\n",
    "        '''does inverse label encoding'''\n",
    "        for col in cols:\n",
    "            if col in label_encoders:\n",
    "                self._inverse_label_encode(df, col)  \n",
    "            else:\n",
    "                raise ValueError(\"label_encoders must be define for each col before calling inverse_encode_df\")\n",
    "\n",
    "    def _label_encode(self, df, col, le=None):\n",
    "        '''label encodes data'''\n",
    "        if le:\n",
    "            df[col] = le.transform(df[col])\n",
    "        else:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(df[col])\n",
    "            df[col] = le.transform(df[col])\n",
    "            self.label_encoders[col] = le\n",
    "            le_dict.get(new_item, '<Unknown>')\n",
    "        \n",
    "    def _inverse_label_encode(self, df, col):\n",
    "        '''inverse label encodes data'''\n",
    "        le = self.label_encoders[col]\n",
    "        df[col] = le.inverse_transform(df[col])\n",
    " \n",
    "    def _create_train_df(self, train_feature_df, train_target_df, preprocess=True, label_encode=True):\n",
    "        '''loads and merges training data features and targets, preprocesses data, encodes data'''\n",
    "        train_feature_df = self._load_data(train_feature_df)\n",
    "        train_target_df = self._load_data(train_target_df)\n",
    "        train_df = self._merge_dfs(train_feature_df, train_target_df)\n",
    "        if preprocess:\n",
    "            train_df = self._clean_data(train_df)\n",
    "            train_df = self._shuffle_data(train_df)\n",
    "        if label_encode:\n",
    "            self.label_encode_df(train_df, self.cat_cols)\n",
    "        return train_df\n",
    "    \n",
    "    def _create_test_df(self, test_file, label_encode=True):\n",
    "        '''loads and label encodes test data'''\n",
    "        test_df = self._load_data(test_file)\n",
    "        if label_encode:\n",
    "            self.label_encode_df(test_df, self.cat_cols)\n",
    "        return test_df\n",
    "        \n",
    "    def _load_data(self, file):\n",
    "        return pd.read_csv(file)\n",
    "    \n",
    "    def _merge_dfs(self, df1, df2, key=None, left_index=True, right_index=True):\n",
    "        return pd.merge(left=df1, right=df2, how='inner', on=key, left_index=left_index, right_index=right_index)\n",
    "    \n",
    "    def _clean_data(self, df):\n",
    "        '''remove rows that contain salary <= 0 or duplicate job IDs'''\n",
    "        df = df.drop_duplicates(subset='id')\n",
    "        #df = df[df.salary>0]\n",
    "        return df\n",
    "    \n",
    "    def _shuffle_data(self, df):\n",
    "         return shuffle(df).reset_index()\n",
    "        \n",
    "    def load_data(file):\n",
    "        return pd.read_csv(file)\n",
    "    \n",
    "    def one_hot_encode_feature_df(df, cat_vars=None, num_vars=None):\n",
    "        '''performs one-hot encoding on all categorical variables and combines result with continuous variables'''\n",
    "        cat_df = pd.get_dummies(df[cat_vars])\n",
    "        num_df = df[num_vars].apply(pd.to_numeric)\n",
    "        return pd.concat([cat_df, num_df], axis=1)#ignore_index=False)\n",
    "    \n",
    "    def split_data(df):\n",
    "        '''splits data into train and test sets'''\n",
    "        train_feature_df, test_df = train_test_split(df, test_size=0.2)\n",
    "        return pd.DataFrame(train_feature_df), pd.DataFrame(test_df)\n",
    "    \n",
    "    def get_target_df(df, target):\n",
    "        '''returns target dataframe'''\n",
    "        return df[target]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pull in the data file and prepare our data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #move working directory to correct location\n",
    "    os.chdir('/Users/timothylmcdonough/DSDJ/NYC_AirBnB/Data')\n",
    "    \n",
    "    #load file\n",
    "    raw_df = Data.load_data('NYC_2019.csv')\n",
    "    \n",
    "    #create dataframe for Bronx only\n",
    "    Bronx_df = pd.DataFrame(raw_df[raw_df['neighbourhood_group'] == 'Bronx'])\n",
    "    \n",
    "    #define variables\n",
    "    cat_cols = ['name', 'host_id', 'host_name', 'neighbourhood_group', 'neighbourhood', 'room_type', 'last_review']\n",
    "    #cat_cols = ['neighbourhood_group', 'neighbourhood', 'room_type']\n",
    "\n",
    "    #numeric_vars = ['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']\n",
    "    num_cols = ['host_id', 'id', 'number_of_reviews', 'latitude', 'longitude', 'mininum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']\n",
    "    target_cols = ['host_id', 'price']\n",
    "    #id_col = 'host_id'\n",
    "\n",
    "    #clean, shuffle, and reindex training data -- shuffling may improve cross-validation accuracy\n",
    "    clean_train_df = shuffle(Bronx_df)\n",
    "    \n",
    "    #get target dataframe\n",
    "    #target_df = Data.get_target_df(Bronx_df, id_col, target_col)\n",
    "    target_df = Data.get_target_df(clean_train_df, target_cols)\n",
    "\n",
    "    #drop columns not helpful to predicitons\n",
    "    #clean_train_df = clean_train_df.drop(columns=['id', 'name', 'host_name', 'room_type', 'last_review', 'latitude', 'longitude', 'minimum_nights', 'last_review', 'reviews_per_month'])\n",
    "    clean_train_df = clean_train_df.drop(columns=['id', 'name', 'host_name', 'room_type', 'last_review', 'latitude', 'longitude', 'minimum_nights', 'last_review', 'calculated_host_listings_count', 'number_of_reviews', 'reviews_per_month', 'availability_365'])\n",
    "\n",
    "    #encode dataset\n",
    "    #raw_encode_df = Data.one_hot_encode_feature_df(raw_df, cat_cols, num_cols)\n",
    "    \n",
    "    #split dataset\n",
    "    train_feature_df, test_df = Data.split_data(clean_train_df)\n",
    "    \n",
    "    #write dataframes to .csv files for modeling Jupyter notebook\n",
    "    train_feature_df.to_csv('train_features.csv', header=True, index=False)\n",
    "    test_df.to_csv('test_features.csv', header=True, index=False)\n",
    "    target_df.to_csv('train_prices.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
